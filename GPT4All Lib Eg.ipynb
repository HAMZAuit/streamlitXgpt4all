{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06992fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gpt4all in c:\\users\\ce pc\\desktop\\gpt4all\\nopenai\\gpt4all_env\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ce pc\\desktop\\gpt4all\\nopenai\\gpt4all_env\\lib\\site-packages (from gpt4all) (4.65.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ce pc\\desktop\\gpt4all\\nopenai\\gpt4all_env\\lib\\site-packages (from gpt4all) (2.30.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ce pc\\desktop\\gpt4all\\nopenai\\gpt4all_env\\lib\\site-packages (from requests->gpt4all) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ce pc\\desktop\\gpt4all\\nopenai\\gpt4all_env\\lib\\site-packages (from requests->gpt4all) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ce pc\\desktop\\gpt4all\\nopenai\\gpt4all_env\\lib\\site-packages (from requests->gpt4all) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ce pc\\desktop\\gpt4all\\nopenai\\gpt4all_env\\lib\\site-packages (from requests->gpt4all) (2.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ce pc\\desktop\\gpt4all\\nopenai\\gpt4all_env\\lib\\site-packages (from tqdm->gpt4all) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c8ea747",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Request failed: HTTP 404 Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt4all\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT4All\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#C:/Users/Ce PC/AppData/Local/nomic.ai/GPT4All/Meta-Llama-3-8B-Instruct.Q4_0.gguf\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT4All\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/Meta-Llama-3-8B-Instruct.Q4_0.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe capital of France is \u001b[39m\u001b[38;5;124m\"\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\gpt4all.py:235\u001b[0m, in \u001b[0;36mGPT4All.__init__\u001b[1;34m(self, model_name, model_path, model_type, allow_download, n_threads, device, n_ctx, ngl, verbose)\u001b[0m\n\u001b[0;32m    232\u001b[0m         device_init \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mremoveprefix(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkompute:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Retrieve model and download if allowed\u001b[39;00m\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig: ConfigType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m LLModel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m], n_ctx, ngl, backend)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\gpt4all.py:341\u001b[0m, in \u001b[0;36mGPT4All.retrieve_model\u001b[1;34m(cls, model_name, model_path, allow_download, verbose)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m allow_download:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;66;03m# If model file does not exist, download\u001b[39;00m\n\u001b[0;32m    340\u001b[0m     filesize \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 341\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilesize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilesize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_md5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmd5sum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel file does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_dest\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\gpt4all.py:393\u001b[0m, in \u001b[0;36mGPT4All.download_model\u001b[1;34m(model_filename, model_path, verbose, url, expected_size, expected_md5)\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected identity Content-Encoding, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m--> 393\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmake_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m total_size_in_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    396\u001b[0m block_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m  \u001b[38;5;66;03m# 1 MB\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\gpt4all.py:386\u001b[0m, in \u001b[0;36mGPT4All.download_model.<locals>.make_request\u001b[1;34m(offset)\u001b[0m\n\u001b[0;32m    384\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m206\u001b[39m):\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRequest failed: HTTP \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset \u001b[38;5;129;01mand\u001b[39;00m (response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m206\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(offset) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Range\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConnection was interrupted and server does not support range requests\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Request failed: HTTP 404 Not Found"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "#C:/Users/Ce PC/AppData/Local/nomic.ai/GPT4All/Meta-Llama-3-8B-Instruct.Q4_0.gguf\n",
    "model = GPT4All(\"models/Meta-Llama-3-8B-Instruct.Q4_0.gguf\")\n",
    "output = model.generate(\"The capital of France is \", max_tokens=3)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a783a54-acb9-4713-a330-838a8568a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt4all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05a0994e-2b57-4960-b1b5-5d5a6c1ffbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'C:/Users/User/AppData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dd77159-97cb-47bc-90a1-2d5bfcfa3a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELPATH= 'Phi-3-mini-4k-instruct.Q4_0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bccab41-cc1d-475e-98b8-fb4ef4343cde",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model filename not in model list: Phi-3-mini-4k-instruct.Q4_0.gguf.bin",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mpt \u001b[38;5;241m=\u001b[39m \u001b[43mgpt4all\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGPT4All\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODELPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\gpt4all.py:47\u001b[0m, in \u001b[0;36mGPT4All.__init__\u001b[1;34m(self, model_name, model_path, model_type, allow_download)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m GPT4All\u001b[38;5;241m.\u001b[39mget_model_from_name(model_name)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Retrieve model and download if allowed\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m model_dest \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_model(model_dest)\n",
      "File \u001b[1;32mc:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\gpt4all.py:108\u001b[0m, in \u001b[0;36mGPT4All.retrieve_model\u001b[1;34m(model_name, model_path, allow_download)\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_match:\n\u001b[1;32m--> 108\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel filename not in model list: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GPT4All\u001b[38;5;241m.\u001b[39mdownload_model(model_filename, model_path)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Model filename not in model list: Phi-3-mini-4k-instruct.Q4_0.gguf.bin"
     ]
    }
   ],
   "source": [
    "mpt = gpt4all.GPT4All(MODELPATH, model_type='mpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b264ceb9-0bd5-49b9-a9f6-63c75233c4dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "message = [{\"role\": \"user\", \"content\": \"Name 3 colors\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74415811-24ec-4eaf-84ac-195973396942",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "task = 'name three colors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "65d573ce-84ad-431c-b77d-a4e6ff1ffe12",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "### Instruction: \n",
    "The prompt below is a question to answer, a task to complete, or a conversation to respond to; decide which and write an appropriate response.\n",
    "### Prompt: \n",
    "{task}\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab1a237d-7ffe-409c-a11c-463e8ae54041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: \n",
      "            The prompt below is a question to answer, a task to complete, or a conversation \n",
      "            to respond to; decide which and write an appropriate response.\n",
      "            \n",
      "### Prompt: \n",
      "Name 3 colors\n",
      "### Response:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt4all-falcon-newbpe-q4_0',\n",
       " 'usage': {'prompt_tokens': 239, 'completion_tokens': 0, 'total_tokens': 239},\n",
       " 'choices': [{'message': {'role': 'assistant', 'content': ''}}]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = mpt.chat_completion(message); response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da0052-14bb-495a-8e48-fda851870332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8baf7e31",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Request failed: HTTP 404 Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt4all\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT4All\n\u001b[0;32m      2\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT4All\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCandle_OrcaMini-3B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\gpt4all.py:235\u001b[0m, in \u001b[0;36mGPT4All.__init__\u001b[1;34m(self, model_name, model_path, model_type, allow_download, n_threads, device, n_ctx, ngl, verbose)\u001b[0m\n\u001b[0;32m    232\u001b[0m         device_init \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mremoveprefix(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkompute:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Retrieve model and download if allowed\u001b[39;00m\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig: ConfigType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m LLModel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m], n_ctx, ngl, backend)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\gpt4all.py:341\u001b[0m, in \u001b[0;36mGPT4All.retrieve_model\u001b[1;34m(cls, model_name, model_path, allow_download, verbose)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m allow_download:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;66;03m# If model file does not exist, download\u001b[39;00m\n\u001b[0;32m    340\u001b[0m     filesize \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 341\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilesize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilesize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_md5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmd5sum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel file does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_dest\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\gpt4all.py:393\u001b[0m, in \u001b[0;36mGPT4All.download_model\u001b[1;34m(model_filename, model_path, verbose, url, expected_size, expected_md5)\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected identity Content-Encoding, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m--> 393\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmake_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m total_size_in_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    396\u001b[0m block_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m  \u001b[38;5;66;03m# 1 MB\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ce PC\\Desktop\\GPT4ALL\\Nopenai\\GPT4ALL_env\\lib\\site-packages\\gpt4all\\gpt4all.py:386\u001b[0m, in \u001b[0;36mGPT4All.download_model.<locals>.make_request\u001b[1;34m(offset)\u001b[0m\n\u001b[0;32m    384\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m206\u001b[39m):\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRequest failed: HTTP \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset \u001b[38;5;129;01mand\u001b[39;00m (response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m206\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(offset) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Range\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConnection was interrupted and server does not support range requests\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Request failed: HTTP 404 Not Found"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "path = \"models/\"\n",
    "model = GPT4All(\"Candle_OrcaMini-3B\", model_path=path, allow_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a7d653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT4ALL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
